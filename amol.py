# -*- coding: utf-8 -*-
"""Amol.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bWNE5PKDNv0M-uDAiASKsuZZGqQb7K7y
"""

import pandas as pd

df=pd.read_csv("/content/NIFTY50_all.csv")

df.head()

df.info()

df.describe()

df.shape

df.isnull().sum()

df['Trades'].fillna(0, inplace=True)
df['Deliverable Volume'].fillna(0, inplace=True)
df['%Deliverble'].fillna(0, inplace=True)

df.isnull().sum()

duplicates = df.duplicated()
df[duplicates]

""" ## Visulization"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(15, 10))

plt.subplot(2, 3, 1)
sns.histplot(df['Open'], kde=True)
plt.title('Distribution of Open Price')
plt.xlabel('Open Price')
plt.ylabel('Frequency')

plt.subplot(2, 3, 2)
sns.histplot(df['High'], kde=True)
plt.title('Distribution of High Price')
plt.xlabel('High Price')
plt.ylabel('Frequency')

plt.subplot(2, 3, 3)
sns.histplot(df['Low'], kde=True)
plt.title('Distribution of Low Price')
plt.xlabel('Low Price')
plt.ylabel('Frequency')

plt.subplot(2, 3, 4)
sns.histplot(df['Close'], kde=True)
plt.title('Distribution of Close Price')
plt.xlabel('Close Price')
plt.ylabel('Frequency')

plt.subplot(2, 3, 5)
sns.histplot(df['Volume'], kde=True)
plt.title('Distribution of Volume')
plt.xlabel('Volume')
plt.ylabel('Frequency')


plt.tight_layout()
plt.show()

numerical_cols = ['Prev Close', 'Open', 'High', 'Low', 'Last', 'Close', 'VWAP', 'Volume', 'Turnover', 'Trades', 'Deliverable Volume', '%Deliverble']
correlation_matrix = df[numerical_cols].corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Financial Attributes')
plt.tight_layout()
plt.show()

symbol_counts = df['Symbol'].value_counts()

plt.figure(figsize=(15, 7))
symbol_counts.plot(kind='bar', color='skyblue')
plt.title('Distribution of Stock Symbols')
plt.xlabel('Stock Symbol')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

average_close_price = df.groupby('Symbol')['Close'].mean().sort_values(ascending=False)

plt.figure(figsize=(15, 7))
average_close_price.plot(kind='bar', color='salmon')
plt.title('Average Closing Price per Stock Symbol')
plt.xlabel('Stock Symbol')
plt.ylabel('Average Closing Price')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

total_volume_per_symbol = df.groupby('Symbol')['Volume'].sum().sort_values(ascending=False)

plt.figure(figsize=(15, 7))
total_volume_per_symbol.plot(kind='bar', color='teal')
plt.title('Total Trading Volume per Stock Symbol')
plt.xlabel('Stock Symbol')
plt.ylabel('Total Volume')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

time_series_df = df[['Date', 'Close']].copy()
display(time_series_df.head())

time_series_df['Date'] = pd.to_datetime(time_series_df['Date'])
time_series_df.set_index('Date', inplace=True)
display(time_series_df.head())

resampled_df = time_series_df.resample('D').mean().ffill()
display(resampled_df.head())

train_size = int(len(resampled_df) * 0.8)
train_data = resampled_df[:train_size]
test_data = resampled_df[train_size:]

display(train_data.head())
display(test_data.head())

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaled_train_data = scaler.fit_transform(train_data)
scaled_test_data = scaler.transform(test_data)

display(scaled_train_data[:5])
display(scaled_test_data[:5])

def create_lag_features(dataframe, window_size):
    df_with_lags = dataframe.copy()
    for i in range(1, window_size + 1):
        df_with_lags[f'Close_Lag_{i}'] = df_with_lags['Close'].shift(i)
    return df_with_lags

def calculate_moving_averages(dataframe, column, windows):
    df_with_ma = dataframe.copy()
    for window in windows:
        df_with_ma[f'{column}_MA_{window}'] = df_with_ma[column].rolling(window=window).mean()
    return df_with_ma

def calculate_rolling_statistics(dataframe, column, windows):
    df_with_rs = dataframe.copy()
    for window in windows:
        df_with_rs[f'{column}_Rolling_Std_{window}'] = df_with_rs[column].rolling(window=window).std()
    return df_with_rs

df_with_lags = create_lag_features(resampled_df, 5)
df_with_lags = calculate_moving_averages(df_with_lags, 'Close', [5, 10, 20])
df_with_lags = calculate_rolling_statistics(df_with_lags, 'Close', [5, 10, 20])

display(df_with_lags.head())

differenced_data = resampled_df['Close'].diff().dropna()
display(differenced_data.head())

import numpy as np

def create_dataset(dataset, look_back_window=1, forecast_horizon=1):
    X, Y = [], []
    for i in range(len(dataset) - look_back_window - forecast_horizon + 1):
        X.append(dataset[i:(i + look_back_window), 0])
        Y.append(dataset[i + look_back_window + forecast_horizon - 1, 0])
    return np.array(X), np.array(Y)

look_back_window = 60
forecast_horizon = 1

X_train, y_train = create_dataset(scaled_train_data, look_back_window, forecast_horizon)
X_test, y_test = create_dataset(scaled_test_data, look_back_window, forecast_horizon)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

print("Shape of X_train:", X_train.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_test:", y_test.shape)

from statsmodels.tsa.arima.model import ARIMA

# Define the ARIMA order (p, d, q)
# For differenced data, d should be 0 as the data is already differenced.
# We are starting with a simple AR(5) model on the differenced data.
arima_order = (5, 0, 0)

# Instantiate and fit the ARIMA model
arima_model = ARIMA(differenced_data, order=arima_order)
arima_results = arima_model.fit()

# Make predictions on the test data length
# We need the length of the test set from the *original* resampled data to predict the correct number of steps.
# The test_data variable still has the correct length based on the original resampled data split.
arima_predictions_diff = arima_results.predict(start=0, end=len(test_data) - 1)

display(arima_predictions_diff.head())

# Inverse the differencing to get predictions in the original scale
# The first value of the test set in the original resampled data is needed as a starting point for inverse differencing.
# We need to align the index of the predictions with the index of the original test data.
arima_predictions = test_data['Close'].iloc[0] + arima_predictions_diff.cumsum()

display(arima_predictions.head())

from sklearn.metrics import mean_squared_error
import numpy as np

mse = mean_squared_error(test_data['Close'], arima_predictions)
rmse = np.sqrt(mse)

print(f"ARIMA Model MSE: {mse}")
print(f"ARIMA Model RMSE: {rmse}")

import matplotlib.pyplot as plt

# Ensure arima_predictions and test_data['Close'] are aligned for plotting
# The index should already be aligned from the previous steps
plt.figure(figsize=(15, 7))
plt.plot(test_data.index, test_data['Close'], label='Actual Values (Test Set)', color='blue')
plt.plot(arima_predictions.index, arima_predictions, label='ARIMA Predictions', color='orange', linestyle='--')
plt.title('ARIMA Model Predictions vs. Actual Values')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

from statsmodels.tsa.statespace.sarimax import SARIMAX

# Define the SARIMA order (p, d, q) and seasonal order (P, D, Q, S)
# Assuming a seasonal period of 7 for weekly seasonality, adjust S if needed.
# The non-seasonal 'd' is 0 as the data is already differenced.
sarima_order = (5, 0, 0)
seasonal_order = (1, 1, 1, 7)

# Instantiate and fit the SARIMA model
sarima_model = SARIMAX(differenced_data, order=sarima_order, seasonal_order=seasonal_order)
sarima_results = sarima_model.fit()

# Make predictions for the length of the original test data
sarima_predictions_diff = sarima_results.predict(start=0, end=len(test_data) - 1)

display(sarima_predictions_diff.head())

# Inverse the differencing to get predictions in the original scale
# We need the first value of the test set in the original resampled data as a starting point for inverse differencing.
# We need to align the index of the predictions with the index of the original test data.
sarima_predictions = test_data['Close'].iloc[0] + sarima_predictions_diff.cumsum()

display(sarima_predictions.head())

from sklearn.metrics import mean_squared_error
import numpy as np

mse_sarima = mean_squared_error(test_data['Close'], sarima_predictions)
rmse_sarima = np.sqrt(mse_sarima)

print(f"SARIMA Model MSE: {mse_sarima}")
print(f"SARIMA Model RMSE: {rmse_sarima}")

import matplotlib.pyplot as plt

# Ensure sarima_predictions and test_data['Close'] are aligned for plotting
# The index should already be aligned from the previous steps
plt.figure(figsize=(15, 7))
plt.plot(test_data.index, test_data['Close'], label='Actual Values (Test Set)', color='blue')
plt.plot(sarima_predictions.index, sarima_predictions, label='SARIMA Predictions', color='green', linestyle='--')
plt.title('SARIMA Model Predictions vs. Actual Values')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Define the LSTM model architecture
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(look_back_window, 1)))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the LSTM model
model.fit(X_train, y_train, epochs=20, batch_size=32)

# Make predictions
scaled_lstm_predictions = model.predict(X_test)

# Inverse transform the predictions
lstm_predictions = scaler.inverse_transform(scaled_lstm_predictions)

print("LSTM Predictions (first 5):")
print(lstm_predictions[:5])

from sklearn.metrics import mean_squared_error
import numpy as np

# Slice the actual values to match the length of LSTM predictions
actual_lstm_values = test_data['Close'][-len(lstm_predictions):]

# Calculate MSE and RMSE for LSTM
mse_lstm = mean_squared_error(actual_lstm_values, lstm_predictions)
rmse_lstm = np.sqrt(mse_lstm)

print(f"LSTM Model MSE: {mse_lstm}")
print(f"LSTM Model RMSE: {rmse_lstm}")

import matplotlib.pyplot as plt

# Ensure actual_lstm_values and lstm_predictions are aligned for plotting
# actual_lstm_values was already sliced to match lstm_predictions length in the previous step
plt.figure(figsize=(15, 7))
plt.plot(actual_lstm_values.index, actual_lstm_values, label='Actual Values (Test Set)', color='blue')
plt.plot(actual_lstm_values.index, lstm_predictions, label='LSTM Predictions', color='red', linestyle='--')
plt.title('LSTM Model Predictions vs. Actual Values')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

prophet_df = resampled_df[['Close']].copy()
prophet_df = prophet_df.rename(columns={'Close': 'y'})
prophet_df = prophet_df.reset_index()
prophet_df = prophet_df.rename(columns={'Date': 'ds'})
display(prophet_df.head())

from prophet import Prophet

# Instantiate a Prophet object
model = Prophet()

# Fit the Prophet model to the prepared data
model.fit(prophet_df)

future = model.make_future_dataframe(periods=len(test_data), freq='D')
display(future.head())
display(future.tail())

forecast = model.predict(future)
display(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head())

# Extract the predicted values for the test data dates
prophet_predictions = forecast[forecast['ds'].isin(test_data.index)]['yhat']

# Ensure actual test data and predictions have the same datetime index
actual_values = test_data['Close'].copy() # Create a copy to avoid SettingWithCopyWarning
prophet_predictions.index = actual_values.index

# Calculate Mean Squared Error (MSE)
mse_prophet = mean_squared_error(actual_values, prophet_predictions)

# Calculate Root Mean Squared Error (RMSE)
rmse_prophet = np.sqrt(mse_prophet)

print(f"Prophet Model MSE: {mse_prophet}")
print(f"Prophet Model RMSE: {rmse_prophet}")

# Ensure 'ds' in forecast is datetime
forecast['ds'] = pd.to_datetime(forecast['ds'])

# Extract the predicted values for the test data dates
prophet_predictions = forecast[forecast['ds'].isin(test_data.index)]['yhat']

# Ensure actual test data and predictions have the same datetime index
actual_values = test_data['Close'].copy() # Create a copy to avoid SettingWithCopyWarning
prophet_predictions.index = actual_values.index

# Calculate Mean Squared Error (MSE)
mse_prophet = mean_squared_error(actual_values, prophet_predictions)

# Calculate Root Mean Squared Error (RMSE)
rmse_prophet = np.sqrt(mse_prophet)

print(f"Prophet Model MSE: {mse_prophet}")
print(f"Prophet Model RMSE: {rmse_prophet}")

# Ensure 'ds' in forecast is datetime
forecast['ds'] = pd.to_datetime(forecast['ds'])

# Extract the predicted values for the test data dates
prophet_predictions = forecast[forecast['ds'].isin(test_data.index)]['yhat']

# Ensure actual test data and predictions have the same datetime index
actual_values = test_data['Close'].copy() # Create a copy to avoid SettingWithCopyWarning
prophet_predictions.index = actual_values.index

# Calculate Mean Squared Error (MSE)
mse_prophet = mean_squared_error(actual_values, prophet_predictions)

# Calculate Root Mean Squared Error (RMSE)
rmse_prophet = np.sqrt(mse_prophet)

print(f"Prophet Model MSE: {mse_prophet}")
print(f"Prophet Model RMSE: {rmse_prophet}")

plt.figure(figsize=(15, 7))
plt.plot(actual_values.index, actual_values, label='Actual Values', color='blue')
plt.plot(prophet_predictions.index, prophet_predictions, label='Prophet Predictions', color='red')
plt.title('Prophet Model Predictions vs. Actual Values')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

print("\n--- Model Comparison ---")
print(f"ARIMA Model RMSE: {rmse}")
print(f"SARIMA Model RMSE: {rmse_sarima}")
print(f"Prophet Model RMSE: {rmse_prophet}")
print(f"LSTM Model RMSE: {rmse_lstm}")

import matplotlib.pyplot as plt

# Ensure all prediction and actual value series are aligned by index for plotting
# actual_lstm_values was created by slicing test_data, so it should be aligned with lstm_predictions
# arima_predictions and sarima_predictions were aligned with test_data.index
# prophet_predictions was also aligned with actual_values (which is a copy of test_data['Close'])

plt.figure(figsize=(18, 10))

plt.plot(test_data.index, test_data['Close'], label='Actual Values (Test Set)', color='blue', linewidth=2)
plt.plot(arima_predictions.index, arima_predictions, label='ARIMA Predictions', color='orange', linestyle='--', linewidth=1)
plt.plot(sarima_predictions.index, sarima_predictions, label='SARIMA Predictions', color='green', linestyle='--', linewidth=1)
# Use actual_lstm_values index for LSTM predictions to ensure alignment after slicing
plt.plot(actual_lstm_values.index, lstm_predictions, label='LSTM Predictions', color='red', linestyle='--', linewidth=1)
plt.plot(prophet_predictions.index, prophet_predictions, label='Prophet Predictions', color='purple', linestyle='--', linewidth=1)


plt.title('Comparison of Model Predictions vs. Actual Values', fontsize=16)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Close Price', fontsize=12)
plt.legend(fontsize=10)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()



"""## Step-by-Step Summary of the Stock Price Forecasting Analysis

This notebook demonstrates a process for analyzing and forecasting stock prices using various time series models.

1.  **Importing Dataset**: The necessary libraries, primarily pandas for data manipulation, are imported, and the stock price data is loaded from a CSV file into a pandas DataFrame.
2.  **Data Exploration and Preprocessing**:
    *   The initial rows and basic information (`df.head()`, `df.info()`) of the DataFrame are displayed to understand the data structure and content.
    *   Descriptive statistics (`df.describe()`) are generated to get an overview of the numerical features.
    *   Missing values (`df.isnull().sum()`) are identified, and a basic imputation strategy (filling with 0) is applied to specific columns ('Trades', 'Deliverable Volume', '%Deliverble').
    *   Duplicate rows (`df.duplicated()`) are checked for.
3.  **Data Visualization**: Various visualizations are created to understand the distribution of key financial attributes (Open, High, Low, Close, Volume) and the distribution of stock symbols. Correlation between numerical features is visualized using a heatmap. The frequency and average closing price per stock symbol are also plotted.
4.  **Time Series Preparation**:
    *   A new DataFrame (`time_series_df`) is created with only the 'Date' and 'Close' columns.
    *   The 'Date' column is converted to datetime objects, and set as the index.
    *   The data is resampled to a daily frequency (`resampled_df`) and missing values are filled using forward fill (`ffill()`).
5.  **Feature Engineering**: Lag features, moving averages, and rolling statistics are calculated for the 'Close' price to capture temporal dependencies and trends.
6.  **Stationarity Check (Differencing)**: The data is differenced to make it stationary, which is often a requirement for time series models like ARIMA.
7.  **Data Splitting and Scaling**:
    *   The resampled time series data is split into training and testing sets.
    *   The data is scaled using `MinMaxScaler` to normalize the values, which is important for models like LSTM.
    *   The data is reshaped into the required format for the LSTM model (samples, time steps, features).
8.  **Model Training and Evaluation**: Four different time series forecasting models are implemented, trained, and evaluated:
    *   **ARIMA**: An ARIMA model is fitted to the differenced data, and predictions are made and then inverse differenced to return to the original scale.
    *   **SARIMA**: A SARIMA model with a seasonal component is fitted to the differenced data, and predictions are inverse differenced.
    *   **LSTM**: A Sequential LSTM model is built, compiled, and trained on the scaled training data. Predictions are made on the scaled test data and then inverse transformed.
    *   **Prophet**: The data is prepared in the format required by Prophet, and a Prophet model is fitted and used to generate future forecasts.
9.  **Model Comparison**: The performance of each model is evaluated using the Root Mean Squared Error (RMSE) metric. The RMSE for each model is printed to compare their accuracy.
10. **Visualization of Predictions**: The predictions from all four models are plotted against the actual values in the test set to visually compare their performance and how well they capture the trends and fluctuations in the stock prices.
"""

